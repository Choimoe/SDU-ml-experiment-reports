	% 代码分析：模块功能、涉及到的类、类关系、数据结构及关键代码等；
	% 任务要求，设计任务要求；
	% 设计：详细的设计方案，相关的数据结构、算法描述，可采用伪代码等形式化描述
	% 实现：修改哪些类、如何修改、为什么修改等；
	% 测试：测试用例，测试结果及结果分析，测试运行界面等；
	% 调试：调试方法，遇到的问题及解决方案等；
	% 结论与展望：完成的主要工作、收获、进一步的工作，建议、体会、心得等；

\section{Experiment 4.1 Regularized Linear Regression}

\subsection{预处理数据}

首先读取数据：

\begin{lstlisting}[language=matlab,title={读入 ex5Lin?.dat}]
clear;
% Load linear regression data
x = load('ex5Linx.dat');
y = load('ex5Liny.dat');
m = length(y); % Number of training examples
\end{lstlisting}

\subsection{数据可视化}

\begin{lstlisting}[language=matlab,title={数据可视化}]
% Visualize the data
figure;
plot(x, y, 'rx', 'MarkerSize', 10); % Scatter plot
xlabel('x');
ylabel('y');
title('Training Data');
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includesvg[width=10cm]{imgs/4-1-1.svg}
\end{figure} 

\subsection{线性回归}

使用五阶多项式：

\[
{h}_{\theta }\left( x\right)  = {\theta }_{0} + {\theta }_{1}x + {\theta }_{2}{x}^{2} + {\theta }_{3}{x}^{3} + {\theta }_{4}{x}^{4} + {\theta }_{5}{x}^{5}
\]

由于我们正在将 5 阶多项式拟合到只有 7 个点的数据集，因此很可能会发生过度拟合。为了防止这种情况，我们将在模型中使用正则化：也就是最小化正则化 Cost：

\[
J\left( \theta \right)  = \frac{1}{2m}\left\lbrack  {\mathop{\sum }\limits_{{i = 1}}^{m}{\left( {h}_{\theta }\left( {x}^{\left( i\right) }\right)  - {y}^{\left( i\right) }\right) }^{2} + \lambda \mathop{\sum }\limits_{{j = 1}}^{n}{\theta }_{j}^{2}}\right\rbrack
\]

在课上学习了正则化线性回归的正则方程解：

\[
\theta  = {\left( {X}^{T}X + \lambda \left\lbrack  \begin{array}{llll} 0 & & & \\   & 1 & & \\   & &  \ddots  & \\   & & & 1 \end{array}\right\rbrack  \right) }^{-1}{X}^{T}\overrightarrow{y}
\]

于是不难写出代码：

\begin{lstlisting}[language=matlab,title={RegularizedLinearRegression.m}]
clear;
% Load linear regression data
x = load('ex5Linx.dat');
y = load('ex5Liny.dat');
m = length(y); % Number of training examples

% Visualize the data
figure;
plot(x, y, 'rx', 'MarkerSize', 10); % Scatter plot
xlabel('x');
ylabel('y');
title('Training Data');

X = [ones(m, 1), x, x.^2, x.^3, x.^4, x.^5];

lambda_vals = [0, 1, 10];
theta_values = zeros(6, length(lambda_vals));

for i = 1:length(lambda_vals)
    lambda = lambda_vals(i);
    L = lambda * diag([0; ones(5, 1)]); % Regularization matrix
    theta = (X' * X + L) \ (X' * y);    % Normal equation with regularization
    theta_values(:, i) = theta;
    
    % Plot the polynomial fit
    x_plot = linspace(min(x), max(x), 100)';
    X_plot = [ones(size(x_plot, 1), 1), x_plot, x_plot.^2, x_plot.^3, x_plot.^4, x_plot.^5];
    y_plot = X_plot * theta;
    
    figure;
    plot(x, y, 'rx', 'MarkerSize', 10);
    hold on;
    plot(x_plot, y_plot, '-b', 'LineWidth', 2);
    xlabel('x');
    ylabel('y');
    title(['Polynomial Fit for \lambda = ', num2str(lambda)]);
end

for i = 1:length(lambda_vals)
    norm_theta = norm(theta_values(:, i));
    fprintf('L2 norm of theta for lambda = %d: %.4f\n', lambda_vals(i), norm_theta);
end
\end{lstlisting}

这里对 $\lambda=\{0,1,10\}$ 进行了计算，可以得到拟合的图像为：

\begin{figure}[H]
    \centering
    \includesvg[width=5.5cm]{imgs/4-1-2.svg}
    \includesvg[width=5.5cm]{imgs/4-1-3.svg}
    \includesvg[width=5.5cm]{imgs/4-1-4.svg}
\end{figure} 

\subsection{question}

\begin{lstlisting}[language=matlab,title={问题描述}]
From looking at these graphs, what conclusions can you make about how the regularization parameter lambda affects your model?
\end{lstlisting}

\begin{itemize}
    \item 较小的 \( \lambda \) 值（ \( \lambda = 0 \)）：本例中模型过度拟合，方差较大。
    \item 适中的 \( \lambda \) 值（ \( \lambda = 1 \)）：本例中看起来还可以
    \item 较大的 \( \lambda \) 值（ \( \lambda = 10 \)）：本例中欠拟合，模型的偏差较大，无法有效拟合数据。
\end{itemize}

\section{Experiment 4.2 Regularized Logistic Regression}

\subsection{预处理数据}

首先读取数据：

\begin{lstlisting}[language=matlab,title={读入 ex5Log?.dat}]
clear;
data = load('ex5Logx.dat');
X = data(:, 1:2);
y = load('ex5Logy.dat');
\end{lstlisting}

\subsection{数据可视化}

\begin{lstlisting}[language=matlab,title={数据可视化}]
figure;
pos = find(y == 1);
neg = find(y == 0);

plot(X(pos, 1), X(pos, 2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);
hold on;
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);
xlabel('Feature u');
ylabel('Feature v');
title('Logistic Regression Data');
legend('Positive', 'Negative');
hold off;
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includesvg[width=10cm]{imgs/4-2-1.svg}
\end{figure} 

\subsection{特征映射}

给出了 map\_feature.m，直接调用来进行多项式特征映射：

\begin{lstlisting}[language=matlab,title={特征映射}]
mapped_X = map_feature(X(:, 1), X(:, 2));
\end{lstlisting}

\subsection{牛顿法求解}

regularized logistic regression 的 cost 函数为：

\[
J\left( \theta \right)  =  - \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}\left\lbrack  {{y}^{\left( i\right) }\log \left( {{h}_{\theta }\left( {x}^{\left( i\right) }\right) }\right)  + \left( {1 - {y}^{\left( i\right) }}\right) \log \left( {1 - {h}_{\theta }\left( {x}^{\left( i\right) }\right) }\right) }\right\rbrack   + \frac{\lambda }{2m}\mathop{\sum }\limits_{{j = 1}}^{n}{\theta }_{j}^{2}
\]

梯度 ${\nabla }_{\theta }J$ 为：

\[
{\nabla }_{\theta }J = \left\lbrack  \begin{matrix} \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}\left( {{h}_{\theta }\left( {x}^{\left( i\right) }\right)  - {y}^{\left( i\right) }}\right) {x}_{0}^{\left( i\right) } \\  \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}\left( {{h}_{\theta }\left( {x}^{\left( i\right) }\right)  - {y}^{\left( i\right) }}\right) {x}_{1}^{\left( i\right) } + \frac{\lambda }{m}{\theta }_{1} \\ \vdots \\  \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}\left( {{h}_{\theta }\left( {x}^{\left( i\right) }\right)  - {y}^{\left( i\right) }}\right) {x}_{1}^{\left( i\right) } + \frac{\lambda }{m}{\theta }_{n} \end{matrix}\right\rbrack
\]

Hessian 矩阵为：

\[
H = \frac{1}{m}\left\lbrack  {\mathop{\sum }\limits_{{i = 1}}^{m}{h}_{\theta }\left( {x}^{\left( i\right) }\right) \left( {1 - {h}_{\theta }\left( {x}^{\left( i\right) }\right) }\right) {x}^{\left( i\right) }{\left( {x}^{\left( i\right) }\right) }^{T}}\right\rbrack   + \frac{\lambda }{m}\left\lbrack  \begin{array}{llll} 0 & & & \\   & 1 & & \\   & &  \ddots  & \\   & & & 1 \end{array}\right\rbrack
\]

\begin{lstlisting}[language=matlab,title={RegularizedLogisticRegression.m}]
initial_theta = zeros(size(mapped_X, 2), 1);

lambda_vals = [0, 1, 10];

for i = 1:length(lambda_vals)
    lambda = lambda_vals(i);
    
    [theta, J_history] = newtons_method(mapped_X, y, initial_theta, lambda);
    
    figure;
    plot_decision_boundary(theta, X, y);
    title(['Decision Boundary with \lambda = ', num2str(lambda)]);
end
\end{lstlisting}

其中 newtons\_method 和 plot\_decision\_boundary 来自我在 ex4 中的代码，由于几乎一样稍微改改就可以使用，于是代码放在结尾了。

这里计算了 $\lambda=\{0,1,10\}$ 的数据，可以画出：

\begin{figure}[H]
    \centering
    \includesvg[width=5.5cm]{imgs/4-2-2.svg}
    \includesvg[width=5.5cm]{imgs/4-2-3.svg}
    \includesvg[width=5.5cm]{imgs/4-2-4.svg}
\end{figure} 

\subsection{question}

\begin{lstlisting}[language=matlab,title={问题描述}]
How does lambda affect the results?
\end{lstlisting}

与前一个实验其实是类似的：

\begin{itemize}
    \item 较小的 \( \lambda \) 值（如 \( \lambda = 0 \)）：本例中导致模型过度拟合，决策边界过于复杂。
    \item 适中的 \( \lambda \) 值（如 \( \lambda = 1 \)）：本例中看起来还好。
    \item 较大的 \( \lambda \) 值（如 \( \lambda = 10 \)）：本例中导致欠拟合，决策边界过于简单，无法有效区分数据。
\end{itemize}

\subsection{完整代码}

\begin{lstlisting}[language=matlab,title={newtons\_method.m}]
function [theta, J_history] = newtons_method(X, y, initial_theta, lambda)
    % X: feature matrix after mapping
    % y: labels
    % initial_theta: initial parameters
    % lambda: regularization parameter

    % Initialize useful values
    [m, n] = size(X); % m = number of training examples, n = number of features
    theta = initial_theta;
    J_history = []; % To store the cost function values

    max_iter = 50;  % Maximum number of iterations
    tolerance = 1e-6;  % Tolerance for convergence

    for iter = 1:max_iter
        % Compute the hypothesis h_theta(x)
        z = X * theta;
        h = sigmoid(z);

        % Compute the cost function J with regularization (excluding theta(1))
        J = -(1/m) * (y' * log(h) + (1 - y)' * log(1 - h)) + ...
            (lambda/(2*m)) * sum(theta(2:end).^2);

        % Gradient (with regularization)
        grad = (1/m) * (X' * (h - y)) + [0; (lambda/m) * theta(2:end)];

        % Hessian (with regularization)
        H = (1/m) * (X' * diag(h .* (1 - h)) * X) + lambda/m * diag([0; ones(n-1, 1)]);

        % Update theta using Newton's method
        delta_theta = H \ grad;
        theta = theta - delta_theta;

        % Store cost function value for each iteration
        J_history = [J_history; J];

        % Check for convergence
        if norm(delta_theta) < tolerance
            fprintf('Converged after %d iterations\n', iter);
            break;
        end
    end
end

% Sigmoid function used in logistic regression
function g = sigmoid(z)
    g = 1 ./ (1 + exp(-z));
end
\end{lstlisting}

\begin{lstlisting}[language=matlab,title={plot\_decision\_boundary.m}]
function plot_decision_boundary(theta, X, y)
    u_vals = linspace(min(X(:,1))-0.1, max(X(:,1))+0.1, 200);
    v_vals = linspace(min(X(:,2))-0.1, max(X(:,2))+0.1, 200);
    
    z = zeros(length(u_vals), length(v_vals));

    for i = 1:length(u_vals)
        for j = 1:length(v_vals)
            z(i, j) = map_feature(u_vals(i), v_vals(j)) * theta;
        end
    end

    contour(u_vals, v_vals, z', [0, 0], 'LineWidth', 2);
    hold on;
    
    pos = find(y == 1);
    neg = find(y == 0);
    plot(X(pos, 1), X(pos, 2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);
    plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);
    
    xlabel('Feature u');
    ylabel('Feature v');
    title('Decision Boundary');
    hold off;
end
\end{lstlisting}